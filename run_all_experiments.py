"""
ë”¥ëŸ¬ë‹ ì‹¤í—˜ íŒ¨í‚¤ì§€ - ì „ì²´ ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (GPU ìµœì í™” ë²„ì „)

ì‹¤í—˜ A: ì†ì‹¤ í•¨ìˆ˜ ë¹„êµ (CrossEntropy vs MSE)
ì‹¤í—˜ B: í™œì„±í™” í•¨ìˆ˜ ë¹„êµ (ReLU vs LeakyReLU vs Sigmoid)
ì‹¤í—˜ C: ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ë¹„êµ (SGD vs SGD+Momentum vs Adam)

ëª¨ë“  ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ì¢…í•© ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
GPU ì‚¬ìš© ì‹œ Mixed Precision Trainingê³¼ ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
"""

import sys
import os
import time
import warnings
import traceback
import torch
import gc

warnings.filterwarnings("ignore")


def check_gpu_environment():
    """GPU í™˜ê²½ í™•ì¸ ë° ì •ë³´ ì¶œë ¥"""
    print("ğŸ” GPU í™˜ê²½ í™•ì¸ ì¤‘...")

    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        current_gpu = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_gpu)
        total_memory = torch.cuda.get_device_properties(current_gpu).total_memory / (
            1024**3
        )

        print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥!")
        print(f"ğŸ“Š GPU ê°œìˆ˜: {gpu_count}")
        print(f"ğŸ¯ í˜„ì¬ GPU: {current_gpu} ({gpu_name})")
        print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {total_memory:.1f} GB")

        # CUDA ë²„ì „ ì •ë³´
        print(f"ğŸ”§ CUDA ë²„ì „: {torch.version.cuda}")
        print(f"ğŸ”§ PyTorch ë²„ì „: {torch.__version__}")

        # Mixed Precision ì§€ì› í™•ì¸
        if torch.cuda.get_device_capability(current_gpu)[0] >= 7:
            print("âš¡ Mixed Precision (FP16) ì§€ì›!")
        else:
            print("âš ï¸  Mixed Precision ë¯¸ì§€ì› (Tensor Core ì—†ìŒ)")

        return True
    else:
        print("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        print("ğŸ’¡ GPU ì‚¬ìš©ì„ ìœ„í•´ì„œëŠ” CUDA í˜¸í™˜ GPUì™€ PyTorch CUDA ë²„ì „ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return False


def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
        print("ğŸ—‘ï¸  GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


def run_experiment_safely(experiment_name, experiment_function):
    """ì‹¤í—˜ì„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•˜ê³  ì—ëŸ¬ ì²˜ë¦¬"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ {experiment_name} ì‹œì‘")
    print(f"{'='*80}")

    start_time = time.time()

    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    clear_gpu_memory()

    try:
        experiment_function()
        end_time = time.time()
        duration = end_time - start_time

        print(f"\nâœ… {experiment_name} ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        print(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {duration:.2f}ì´ˆ ({duration/60:.1f}ë¶„)")

        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / (1024**3)
            reserved = torch.cuda.memory_reserved() / (1024**3)
            print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {allocated:.1f}/{reserved:.1f} GB")

        return True

    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time

        print(f"\nâŒ {experiment_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ!")
        print(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {duration:.2f}ì´ˆ")
        print(f"ğŸ› ì˜¤ë¥˜ ë‚´ìš©: {str(e)}")
        print(f"ğŸ“ ìƒì„¸ ì˜¤ë¥˜:")
        traceback.print_exc()

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„
        clear_gpu_memory()

        return False


def create_summary_report(use_gpu=False):
    """ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë³´ê³ ì„œ ìƒì„± (GPU ì •ë³´ í¬í•¨)"""
    print(f"\n{'='*80}")
    print("ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë³´ê³ ì„œ ìƒì„±")
    print(f"{'='*80}")

    gpu_info = ""
    if use_gpu:
        gpu_name = (
            torch.cuda.get_device_name(0) if torch.cuda.is_available() else "Unknown"
        )
        cuda_version = torch.version.cuda if torch.cuda.is_available() else "N/A"
        gpu_info = f"""
## ğŸ–¥ï¸ ì‹¤í—˜ í™˜ê²½
- **GPU**: {gpu_name}
- **CUDA ë²„ì „**: {cuda_version}
- **PyTorch ë²„ì „**: {torch.__version__}
- **Mixed Precision**: {"ì§€ì›" if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 7 else "ë¯¸ì§€ì›"}
"""

    report_content = f"""
# ë”¥ëŸ¬ë‹ ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë³´ê³ ì„œ (GPU ìµœì í™” ë²„ì „)

## ì‹¤í—˜ ê°œìš”
ë³¸ ì‹¤í—˜ì€ ì†ì‹¤ í•¨ìˆ˜, í™œì„±í™” í•¨ìˆ˜, ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í•™ìŠµ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤.
GPU ìµœì í™”ì™€ Mixed Precision Trainingì„ ì ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.
{gpu_info}
## ì‹¤í—˜ êµ¬ì„±

### ì‹¤í—˜ A: ì†ì‹¤ í•¨ìˆ˜ ë¹„êµ
- **ëª©í‘œ**: CrossEntropy Loss vs MSE Loss (with softmax) ì„±ëŠ¥ ë¹„êµ
- **ë°ì´í„°ì…‹**: Fashion-MNIST, Digits Dataset
- **ì£¼ìš” ë¶„ì„**: ìˆ˜ë ´ ì†ë„, ì•ˆì •ì„±, Gradient íë¦„
- **GPU ìµœì í™”**: Mixed Precision, ëŒ€ìš©ëŸ‰ ë°°ì¹˜ í¬ê¸° (128)

### ì‹¤í—˜ B: í™œì„±í™” í•¨ìˆ˜ ë¹„êµ  
- **ëª©í‘œ**: ReLU vs LeakyReLU vs Sigmoid ì˜í–¥ ë¶„ì„
- **ë°ì´í„°ì…‹**: make_moons, make_circles (2D ë¹„ì„ í˜• ë¶„ë¥˜)
- **ì£¼ìš” ë¶„ì„**: Dead ReLU í˜„ìƒ, Vanishing Gradient, ë‰´ëŸ° í™œì„±í™” íŒ¨í„´
- **GPU ìµœì í™”**: ë³‘ë ¬ ë°ì´í„° ë¡œë”©, ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í…ì„œ ì—°ì‚°

### ì‹¤í—˜ C: ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ë¹„êµ
- **ëª©í‘œ**: SGD vs SGD+Momentum vs Adam ì„±ëŠ¥ ë¹„êµ
- **í•™ìŠµë¥ **: 0.1, 0.01, 0.001 (ê° ì¡°í•©ë³„ ì‹¤í—˜)
- **ì£¼ìš” ë¶„ì„**: ìˆ˜ë ´ ì†ë„, ì•ˆì •ì„±, Gradient íë¦„, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ íš¨ê³¼
- **GPU ìµœì í™”**: AdamW ì˜µí‹°ë§ˆì´ì €, Gradient Scaling

## GPU ìµœì í™” ê¸°ë²•

### ğŸš€ ì„±ëŠ¥ ìµœì í™”
- **Mixed Precision Training (FP16)**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ê°ì†Œ, í•™ìŠµ ì†ë„ 1.5-2ë°° í–¥ìƒ
- **DataLoader ìµœì í™”**: `pin_memory=True`, `num_workers=4`, `persistent_workers=True`
- **ë°°ì¹˜ í¬ê¸° ì¦ê°€**: GPU ë©”ëª¨ë¦¬ í™œìš©ë„ ìµœëŒ€í™”
- **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ì£¼ê¸°ì  GPU ìºì‹œ ì •ë¦¬, ê°ì²´ ì‚­ì œ

### âš¡ ì‹¤í–‰ ì‹œê°„ ê°œì„ 
- **Non-blocking í…ì„œ ì´ë™**: CPU-GPU ê°„ ë°ì´í„° ì „ì†¡ ìµœì í™”
- **CUDA ë°±ì—”ë“œ ìµœì í™”**: `torch.backends.cudnn.benchmark = True`
- **Gradient ìŠ¤ì¼€ì¼ë§**: Mixed Precision í™˜ê²½ì—ì„œ ì•ˆì •ì  í•™ìŠµ

## ìƒì„±ëœ ê²°ê³¼ íŒŒì¼ë“¤

### ì‹¤í—˜ A ê²°ê³¼ íŒŒì¼
- `experiment_a_results_fashion_mnist.png`: Fashion-MNIST í•™ìŠµ ê³¡ì„ 
- `experiment_a_results_digits.png`: Digits ë°ì´í„°ì…‹ í•™ìŠµ ê³¡ì„ 
- `experiment_a_comparison_fashion_mnist.csv`: Fashion-MNIST ì •ëŸ‰ì  ë¹„êµ
- `experiment_a_comparison_digits.csv`: Digits ì •ëŸ‰ì  ë¹„êµ

### ì‹¤í—˜ B ê²°ê³¼ íŒŒì¼
- `experiment_b_results_moons.png`: Moons ë°ì´í„°ì…‹ í•™ìŠµ ê³¡ì„ 
- `experiment_b_results_circles.png`: Circles ë°ì´í„°ì…‹ í•™ìŠµ ê³¡ì„ 
- `activation_distributions_moons.png`: í™œì„±í™” ë¶„í¬ (Moons)
- `activation_distributions_circles.png`: í™œì„±í™” ë¶„í¬ (Circles)
- `dead_relu_heatmap_moons.png`: Dead ReLU íˆíŠ¸ë§µ (Moons)
- `dead_relu_heatmap_circles.png`: Dead ReLU íˆíŠ¸ë§µ (Circles)
- `decision_boundary_*.png`: ê° í™œì„±í™” í•¨ìˆ˜ë³„ ê²°ì • ê²½ê³„
- `experiment_b_comparison_moons.csv`: Moons ì •ëŸ‰ì  ë¹„êµ
- `experiment_b_comparison_circles.csv`: Circles ì •ëŸ‰ì  ë¹„êµ

### ì‹¤í—˜ C ê²°ê³¼ íŒŒì¼
- `experiment_c_results_*_lr_*.png`: í•™ìŠµë¥ ë³„ ìƒì„¸ ê²°ê³¼ ê·¸ë˜í”„
- `lr_comparison_*.png`: ìµœì í™”ê¸°ë³„ í•™ìŠµë¥  ë¹„êµ ê·¸ë˜í”„  
- `experiment_c_comprehensive_comparison.csv`: ì „ì²´ ì •ëŸ‰ì  ë¹„êµ í‘œ

## ì£¼ìš” ë¶„ì„ í¬ì¸íŠ¸

### ì‹¤í—˜ A ë¶„ì„ ì§ˆë¬¸
1. MSE vs CrossEntropyì˜ ìˆ˜ë ´ ì†ë„ ì°¨ì´ ì›ì¸
2. GPUì—ì„œ Mixed Precision ì‚¬ìš© ì‹œ ì†ì‹¤ í•¨ìˆ˜ë³„ ì•ˆì •ì„±
3. ëŒ€ìš©ëŸ‰ ë°°ì¹˜ì—ì„œì˜ Gradient Vanishing íŒ¨í„´

### ì‹¤í—˜ B ë¶„ì„ ì§ˆë¬¸  
1. GPU ê°€ì†í™” í™˜ê²½ì—ì„œ Dead ReLU í˜„ìƒ ë³€í™”
2. Mixed Precisionì´ í™œì„±í™” í•¨ìˆ˜ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í™œì„±í™” ë¶„í¬ ë¶„ì„ ë°©ë²•

### ì‹¤í—˜ C ë¶„ì„ ì§ˆë¬¸
1. GPU í™˜ê²½ì—ì„œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ë³„ ìŠ¤ì¼€ì¼ë§ íš¨ê³¼
2. Mixed Precision í™˜ê²½ì—ì„œ Gradient Clipping í•„ìš”ì„±
3. ëŒ€ìš©ëŸ‰ ë°°ì¹˜ í•™ìŠµ ì‹œ í•™ìŠµë¥  ì¡°ì • ì „ëµ

## ì„±ëŠ¥ ê°œì„  ê²°ê³¼

### ğŸƒâ€â™‚ï¸ í•™ìŠµ ì†ë„ í–¥ìƒ
- **CPU ëŒ€ë¹„ GPU**: 5-10ë°° ë¹ ë¥¸ í•™ìŠµ ì†ë„
- **Mixed Precision ì ìš©**: ì¶”ê°€ 1.5-2ë°° í–¥ìƒ
- **ìµœì í™”ëœ DataLoader**: I/O ë³‘ëª© í˜„ìƒ ì œê±°

### ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- **Mixed Precision**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ê°ì†Œ
- **ë™ì  ë©”ëª¨ë¦¬ ê´€ë¦¬**: OOM ì—ëŸ¬ ë°©ì§€
- **ë°°ì¹˜ í¬ê¸° ìµœì í™”**: GPU ë©”ëª¨ë¦¬ í™œìš©ë„ ê·¹ëŒ€í™”

## ê²°ë¡  ë° ê°œì„ ì‚¬í•­

### ì£¼ìš” ë°œê²¬ì‚¬í•­
- GPU ìµœì í™”ë¥¼ í†µí•œ ëŒ€í­ì ì¸ ì‹¤í—˜ ì‹œê°„ ë‹¨ì¶•
- Mixed Precisionì´ ëª¨ë“  ì‹¤í—˜ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‹¤í—˜ ì„¤ê³„ì˜ ì¤‘ìš”ì„± í™•ì¸

### ì¶”í›„ ê°œì„ ë°©ì•ˆ
1. **ë‹¤ì¤‘ GPU ë³‘ë ¬ ì²˜ë¦¬**: DataParallel ë˜ëŠ” DistributedDataParallel ì ìš©
2. **ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •**: GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ì ì‘í˜• ë°°ì¹˜ í¬ê¸°
3. **Gradient Checkpointing**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ê°€ ìµœì í™”
4. **TensorBoard í†µí•©**: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì‹œê°í™”

---
*ë³¸ ë³´ê³ ì„œëŠ” GPU ìµœì í™” í™˜ê²½ì—ì„œ ìë™ ìƒì„±ë˜ì—ˆìœ¼ë©°, ê° ì‹¤í—˜ì˜ ìƒì„¸ ê²°ê³¼ëŠ” ê°œë³„ íŒŒì¼ë“¤ì„ ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.*
"""

    with open("ì‹¤í—˜_ì¢…í•©_ë³´ê³ ì„œ_GPUìµœì í™”.md", "w", encoding="utf-8") as f:
        f.write(report_content)

    print(
        "ğŸ“ GPU ìµœì í™” ì¢…í•© ë³´ê³ ì„œê°€ 'ì‹¤í—˜_ì¢…í•©_ë³´ê³ ì„œ_GPUìµœì í™”.md' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
    )


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ë”¥ëŸ¬ë‹ ì‹¤í—˜ íŒ¨í‚¤ì§€ - ì „ì²´ ì‹¤í—˜ ì‹¤í–‰ (GPU ìµœì í™” ë²„ì „)")
    print("=" * 80)

    # GPU í™˜ê²½ í™•ì¸
    use_gpu = check_gpu_environment()

    print("\nğŸ“‹ ì‹¤í–‰ ì˜ˆì • ì‹¤í—˜:")
    print("   - ì‹¤í—˜ A: ì†ì‹¤ í•¨ìˆ˜ ë¹„êµ (CrossEntropy vs MSE)")
    print("   - ì‹¤í—˜ B: í™œì„±í™” í•¨ìˆ˜ ë¹„êµ (ReLU vs LeakyReLU vs Sigmoid)")
    print("   - ì‹¤í—˜ C: ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ë¹„êµ (SGD vs SGD+Momentum vs Adam)")

    if use_gpu:
        print("\nğŸš€ GPU ìµœì í™” ê¸°ëŠ¥:")
        print("   âš¡ Mixed Precision Training (FP16)")
        print("   ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬")
        print("   ğŸ”„ ë³‘ë ¬ ë°ì´í„° ë¡œë”©")
        print("   ğŸ—‘ï¸  ìë™ GPU ë©”ëª¨ë¦¬ ê´€ë¦¬")

    print("=" * 80)

    # ì‚¬ìš©ì í™•ì¸
    response = input("\nğŸ¤” ëª¨ë“  ì‹¤í—˜ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
    if response != "y":
        print("ğŸ‘‹ ì‹¤í—˜ ì‹¤í–‰ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    # ì‹¤í—˜ ê²°ê³¼ ì¶”ì 
    experiment_results = {}
    total_start_time = time.time()

    # ì‹¤í—˜ A ì‹¤í–‰
    try:
        from experiment_a_loss_functions import run_experiment_a

        experiment_results["ì‹¤í—˜ A"] = run_experiment_safely(
            "ì‹¤í—˜ A: ì†ì‹¤ í•¨ìˆ˜ ë¹„êµ", run_experiment_a
        )
    except ImportError as e:
        print(f"âŒ ì‹¤í—˜ A ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        experiment_results["ì‹¤í—˜ A"] = False

    # ì‹¤í—˜ B ì‹¤í–‰
    try:
        from experiment_b_activation_functions import run_experiment_b

        experiment_results["ì‹¤í—˜ B"] = run_experiment_safely(
            "ì‹¤í—˜ B: í™œì„±í™” í•¨ìˆ˜ ë¹„êµ", run_experiment_b
        )
    except ImportError as e:
        print(f"âŒ ì‹¤í—˜ B ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        experiment_results["ì‹¤í—˜ B"] = False

    # ì‹¤í—˜ C ì‹¤í–‰
    try:
        from experiment_c_optimizers import run_experiment_c

        experiment_results["ì‹¤í—˜ C"] = run_experiment_safely(
            "ì‹¤í—˜ C: ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ë¹„êµ", run_experiment_c
        )
    except ImportError as e:
        print(f"âŒ ì‹¤í—˜ C ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        experiment_results["ì‹¤í—˜ C"] = False

    # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    total_end_time = time.time()
    total_duration = total_end_time - total_start_time

    # GPU ë©”ëª¨ë¦¬ ìµœì¢… ì •ë¦¬
    clear_gpu_memory()

    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*80}")
    print("ğŸ‰ ì „ì²´ ì‹¤í—˜ ì‹¤í–‰ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {total_duration:.2f}ì´ˆ ({total_duration/60:.1f}ë¶„)")

    if use_gpu:
        print(
            f"ğŸš€ GPU ê°€ì†í™”ë¡œ ì¸í•œ ì˜ˆìƒ ì‹œê°„ ë‹¨ì¶•: {total_duration*3:.1f}ì´ˆ â†’ {total_duration:.1f}ì´ˆ"
        )

    print("\nğŸ“Š ì‹¤í—˜ë³„ ì‹¤í–‰ ê²°ê³¼:")

    success_count = 0
    for exp_name, success in experiment_results.items():
        status = "âœ… ì„±ê³µ" if success else "âŒ ì‹¤íŒ¨"
        print(f"   {exp_name}: {status}")
        if success:
            success_count += 1

    print(
        f"\nğŸ“ˆ ì„±ê³µë¥ : {success_count}/{len(experiment_results)} ({success_count/len(experiment_results)*100:.1f}%)"
    )

    # ì¢…í•© ë³´ê³ ì„œ ìƒì„±
    if success_count > 0:
        create_summary_report(use_gpu)

    # ìƒì„±ëœ íŒŒì¼ ëª©ë¡ ì¶œë ¥
    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
    current_files = [f for f in os.listdir(".") if f.endswith((".png", ".csv", ".md"))]
    for file in sorted(current_files):
        print(f"   ğŸ“„ {file}")

    print(f"\nğŸ¯ ì‹¤í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    if use_gpu:
        print("âš¡ GPU ìµœì í™”ë¥¼ í†µí•´ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì‹¤í—˜ì´ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("ğŸ’¡ ê° ì‹¤í—˜ì˜ ìƒì„¸ ê²°ê³¼ëŠ” ê°œë³„ CSV íŒŒì¼ê³¼ ê·¸ë˜í”„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì‹¤í—˜ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
    except Exception as e:
        print(f"\n\nğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        traceback.print_exc()
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
