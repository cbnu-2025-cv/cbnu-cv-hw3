"""
ì‹¤í—˜ B: í™œì„±í™” í•¨ìˆ˜ ë¹„êµ (ReLU vs LeakyReLU vs Sigmoid)

ëª©í‘œ: ReLU, LeakyReLU, Sigmoidê°€ í•™ìŠµì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„
- Dead ReLU ë°œìƒ ìœ ë„ ë° LeakyReLUì˜ ì™„í™” íš¨ê³¼ í™•ì¸
- Sigmoidì˜ vanishing gradient ë¬¸ì œ ë¶„ì„
- Layerë³„ ì¶œë ¥ê°’ ë° ë‰´ëŸ° í™œì„±í™” íŒ¨í„´ ì‹œê°í™”
"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.datasets import make_moons, make_circles
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
from tqdm import tqdm
import warnings
import gc

warnings.filterwarnings("ignore")

# ì‹œê°í™” ì„¤ì •
plt.rcParams["font.family"] = "DejaVu Sans"
sns.set_style("whitegrid")


# GPU ì„¤ì • ìµœì í™”
def setup_device():
    """GPU ì„¤ì • ë° ìµœì í™”"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        # CUDA ìµœì í™” ì„¤ì •
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

        # GPU ì •ë³´ ì¶œë ¥
        gpu_count = torch.cuda.device_count()
        current_gpu = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_gpu)

        print(f"ğŸš€ GPU ì‚¬ìš© ê°€ëŠ¥!")
        print(f"ğŸ“Š GPU ê°œìˆ˜: {gpu_count}")
        print(f"ğŸ¯ í˜„ì¬ GPU: {current_gpu} ({gpu_name})")

        # GPU ë©”ëª¨ë¦¬ ì •ë³´
        total_memory = torch.cuda.get_device_properties(current_gpu).total_memory / (
            1024**3
        )
        print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {total_memory:.1f} GB")

        return device, True
    else:
        print("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return torch.device("cpu"), False


def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()


class MLPWithActivation(nn.Module):
    """í™œì„±í™” í•¨ìˆ˜ë¥¼ ë³€ê²½ ê°€ëŠ¥í•œ MLP ë„¤íŠ¸ì›Œí¬"""

    def __init__(self, input_size, num_classes, activation_fn, small_weights=True):
        super(MLPWithActivation, self).__init__()

        self.layer1 = nn.Linear(input_size, 256)
        self.layer2 = nn.Linear(256, 128)
        self.layer3 = nn.Linear(128, num_classes)

        # í™œì„±í™” í•¨ìˆ˜ ì„¤ì •
        if activation_fn == "relu":
            self.activation = nn.ReLU()
        elif activation_fn == "leaky_relu":
            self.activation = nn.LeakyReLU(negative_slope=0.01)
        elif activation_fn == "sigmoid":
            self.activation = nn.Sigmoid()

        # Dead ReLU ìœ ë„ë¥¼ ìœ„í•œ ì‘ì€ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        if small_weights:
            self._initialize_small_weights()

    def _initialize_small_weights(self):
        """ì‘ì€ ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”í•˜ì—¬ Dead ReLU ìƒí™© ìœ ë„"""
        for layer in [self.layer1, self.layer2, self.layer3]:
            nn.init.normal_(layer.weight, mean=0.0, std=0.01)
            nn.init.zeros_(layer.bias)

    def forward(self, x):
        # ì¤‘ê°„ ë ˆì´ì–´ ì¶œë ¥ê°’ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        activations = []

        x = self.layer1(x)
        x = self.activation(x)
        activations.append(x.clone())

        x = self.layer2(x)
        x = self.activation(x)
        activations.append(x.clone())

        x = self.layer3(x)

        return x, activations


def load_2d_datasets(use_gpu=True):
    """2D ë¹„ì„ í˜• ë¶„ë¥˜ ë°ì´í„°ì…‹ ìƒì„± (GPU ìµœì í™”)"""
    datasets = {}

    # make_moons ë°ì´í„°ì…‹
    X_moons, y_moons = make_moons(n_samples=1000, noise=0.3, random_state=42)
    scaler = StandardScaler()
    X_moons = scaler.fit_transform(X_moons)

    X_train_moons, X_test_moons, y_train_moons, y_test_moons = train_test_split(
        X_moons, y_moons, test_size=0.3, random_state=42, stratify=y_moons
    )

    # GPUì—ì„œ ì‚¬ìš©í•  ê²½ìš° í…ì„œë¥¼ ë¯¸ë¦¬ GPUë¡œ ì´ë™
    device_type = "cuda" if use_gpu and torch.cuda.is_available() else "cpu"

    datasets["moons"] = {
        "X_train": torch.FloatTensor(X_train_moons),
        "X_test": torch.FloatTensor(X_test_moons),
        "y_train": torch.LongTensor(y_train_moons),
        "y_test": torch.LongTensor(y_test_moons),
        "input_size": 2,
        "num_classes": 2,
    }

    # make_circles ë°ì´í„°ì…‹
    X_circles, y_circles = make_circles(
        n_samples=1000, noise=0.2, factor=0.5, random_state=42
    )
    scaler = StandardScaler()
    X_circles = scaler.fit_transform(X_circles)

    X_train_circles, X_test_circles, y_train_circles, y_test_circles = train_test_split(
        X_circles, y_circles, test_size=0.3, random_state=42, stratify=y_circles
    )

    datasets["circles"] = {
        "X_train": torch.FloatTensor(X_train_circles),
        "X_test": torch.FloatTensor(X_test_circles),
        "y_train": torch.LongTensor(y_train_circles),
        "y_test": torch.LongTensor(y_test_circles),
        "input_size": 2,
        "num_classes": 2,
    }

    return datasets


def calculate_dead_relu_ratio(activations):
    """Dead ReLU ë¹„ìœ¨ ê³„ì‚°"""
    dead_ratios = []

    for layer_activation in activations:
        # 0 ì´í•˜ì˜ ê°’ì„ ê°€ì§„ ë‰´ëŸ° ë¹„ìœ¨ ê³„ì‚°
        total_neurons = layer_activation.numel()
        dead_neurons = (layer_activation <= 0).sum().item()
        dead_ratio = dead_neurons / total_neurons * 100
        dead_ratios.append(dead_ratio)

    return dead_ratios


def train_model_with_analysis(
    model, data, epochs, learning_rate, device, activation_name, use_amp=True
):
    """í™œì„±í™” í•¨ìˆ˜ ë¶„ì„ì„ í¬í•¨í•œ ëª¨ë¸ í›ˆë ¨ (Mixed Precision ì§€ì›)"""
    model.to(device)

    X_train, X_test = data["X_train"].to(device, non_blocking=True), data["X_test"].to(
        device, non_blocking=True
    )
    y_train, y_test = data["y_train"].to(device, non_blocking=True), data["y_test"].to(
        device, non_blocking=True
    )

    # GPU ì‚¬ìš© ì‹œ AdamW ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© (ë” ì•ˆì •ì )
    if device.type == "cuda":
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    else:
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    criterion = nn.CrossEntropyLoss()

    # Mixed Precision ì„¤ì •
    scaler = torch.cuda.amp.GradScaler() if use_amp and device.type == "cuda" else None

    # ê¸°ë¡ìš© ë¦¬ìŠ¤íŠ¸ë“¤
    train_losses = []
    train_accuracies = []
    test_losses = []
    test_accuracies = []
    dead_relu_history = []  # Dead ReLU ë¹„ìœ¨ ê¸°ë¡
    activation_distributions = []  # í™œì„±í™” ë¶„í¬ ê¸°ë¡

    for epoch in tqdm(range(epochs), desc=f"Training with {activation_name}"):
        # í›ˆë ¨ ëª¨ë“œ
        model.train()

        # Mixed Precision Forward Pass
        if use_amp and device.type == "cuda":
            with torch.cuda.amp.autocast():
                outputs, activations = model(X_train)
                loss = criterion(outputs, y_train)

            optimizer.zero_grad(set_to_none=True)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs, activations = model(X_train)
            loss = criterion(outputs, y_train)

            optimizer.zero_grad(set_to_none=True)
            loss.backward()
            optimizer.step()

        # í›ˆë ¨ ì •í™•ë„ ê³„ì‚°
        _, predicted = torch.max(outputs.data, 1)
        train_acc = (predicted == y_train).sum().item() / y_train.size(0) * 100

        train_losses.append(loss.item())
        train_accuracies.append(train_acc)

        # í…ŒìŠ¤íŠ¸ í‰ê°€
        model.eval()
        with torch.no_grad():
            # Mixed Precision Inference
            if use_amp and device.type == "cuda":
                with torch.cuda.amp.autocast():
                    test_outputs, test_activations = model(X_test)
                    test_loss = criterion(test_outputs, y_test)
            else:
                test_outputs, test_activations = model(X_test)
                test_loss = criterion(test_outputs, y_test)

            _, test_predicted = torch.max(test_outputs.data, 1)
            test_acc = (test_predicted == y_test).sum().item() / y_test.size(0) * 100

        test_losses.append(test_loss.item())
        test_accuracies.append(test_acc)

        # Dead ReLU ë¶„ì„ (ReLU ê³„ì—´ í™œì„±í™” í•¨ìˆ˜ì— ëŒ€í•´ì„œë§Œ)
        if activation_name in ["ReLU", "LeakyReLU"]:
            dead_ratios = calculate_dead_relu_ratio(activations)
            dead_relu_history.append(dead_ratios)

        # í™œì„±í™” ë¶„í¬ ì €ì¥ (íŠ¹ì • ì—í¬í¬ì—ë§Œ) - ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ CPUë¡œ ì´ë™
        if epoch % (epochs // 5) == 0:  # 5ê°œì˜ ì‹œì ì—ì„œ ê¸°ë¡
            activation_dist = []
            for layer_act in activations:
                activation_dist.append(layer_act.detach().cpu().numpy().flatten())
            activation_distributions.append(
                {"epoch": epoch, "distributions": activation_dist}
            )

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ë§¤ 50 ì—í¬í¬ë§ˆë‹¤)
        if epoch % 50 == 0 and device.type == "cuda":
            clear_gpu_memory()

    return {
        "train_losses": train_losses,
        "train_accuracies": train_accuracies,
        "test_losses": test_losses,
        "test_accuracies": test_accuracies,
        "dead_relu_history": dead_relu_history,
        "activation_distributions": activation_distributions,
        "model": model,
    }


def analyze_gradients(model, data, device):
    """ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ë¶„ì„"""
    model.train()

    X_train = data["X_train"].to(device, non_blocking=True)
    y_train = data["y_train"].to(device, non_blocking=True)

    criterion = nn.CrossEntropyLoss()

    outputs, _ = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()

    gradients = []
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.abs().mean().item()
            gradients.append((name, grad_norm))

    return gradients


def plot_2d_decision_boundary(model, data, dataset_name, activation_name):
    """2D ë°ì´í„°ì˜ ê²°ì • ê²½ê³„ ì‹œê°í™”"""
    model.eval()

    X_test = data["X_test"].cpu().numpy()  # CPUë¡œ ì´ë™í•˜ì—¬ ì‹œê°í™”
    y_test = data["y_test"].cpu().numpy()

    # ê·¸ë¦¬ë“œ ìƒì„±
    h = 0.02
    x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1
    y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    grid_points = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])

    # GPUì—ì„œ ì¶”ë¡ í•˜ê³  CPUë¡œ ê²°ê³¼ ì´ë™
    if next(model.parameters()).is_cuda:
        grid_points = grid_points.cuda()

    with torch.no_grad():
        Z, _ = model(grid_points)
        Z = torch.softmax(Z, dim=1)[:, 1].cpu().numpy()

    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(10, 8))
    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap=plt.cm.RdYlBu)
    scatter = plt.scatter(
        X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.RdYlBu, edgecolors="black"
    )
    plt.colorbar(scatter)
    plt.title(f"Decision Boundary: {activation_name} on {dataset_name}")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.savefig(
        f"decision_boundary_{activation_name}_{dataset_name}.png",
        dpi=300,
        bbox_inches="tight",
    )
    plt.show()


def plot_activation_distributions(results_dict, dataset_name):
    """í™œì„±í™” í•¨ìˆ˜ë³„ ë‰´ëŸ° ì¶œë ¥ ë¶„í¬ ì‹œê°í™”"""
    fig, axes = plt.subplots(3, 2, figsize=(15, 12))
    fig.suptitle(f"Layerë³„ í™œì„±í™” ë¶„í¬ ({dataset_name})", fontsize=16)

    for idx, (activation_name, results) in enumerate(results_dict.items()):
        if not results["activation_distributions"]:
            continue

        # ë§ˆì§€ë§‰ ì—í¬í¬ì˜ í™œì„±í™” ë¶„í¬ ì‚¬ìš©
        last_dist = results["activation_distributions"][-1]["distributions"]

        # Layer 1
        axes[idx, 0].hist(last_dist[0], bins=50, alpha=0.7, density=True)
        axes[idx, 0].set_title(f"{activation_name} - Layer 1")
        axes[idx, 0].set_xlabel("Activation Value")
        axes[idx, 0].set_ylabel("Density")

        # Layer 2
        axes[idx, 1].hist(last_dist[1], bins=50, alpha=0.7, density=True)
        axes[idx, 1].set_title(f"{activation_name} - Layer 2")
        axes[idx, 1].set_xlabel("Activation Value")
        axes[idx, 1].set_ylabel("Density")

    plt.tight_layout()
    plt.savefig(
        f"activation_distributions_{dataset_name}.png", dpi=300, bbox_inches="tight"
    )
    plt.show()


def plot_dead_relu_heatmap(results_dict, dataset_name):
    """Dead ReLU ë¹„ìœ¨ íˆíŠ¸ë§µ ì‹œê°í™”"""
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    fig.suptitle(f"Dead ReLU ë¹„ìœ¨ ë³€í™” ({dataset_name})", fontsize=16)

    for idx, (activation_name, results) in enumerate(results_dict.items()):
        if (
            activation_name not in ["ReLU", "LeakyReLU"]
            or not results["dead_relu_history"]
        ):
            continue

        # Dead ReLU íˆìŠ¤í† ë¦¬ë¥¼ ë°°ì—´ë¡œ ë³€í™˜
        dead_relu_array = np.array(results["dead_relu_history"])

        # íˆíŠ¸ë§µ ìƒì„±
        im = axes[idx].imshow(dead_relu_array.T, cmap="Reds", aspect="auto")
        axes[idx].set_title(f"{activation_name} Dead Neuron Ratio (%)")
        axes[idx].set_xlabel("Epoch")
        axes[idx].set_ylabel("Layer")
        axes[idx].set_yticks([0, 1])
        axes[idx].set_yticklabels(["Layer 1", "Layer 2"])

        # ì»¬ëŸ¬ë°” ì¶”ê°€
        plt.colorbar(im, ax=axes[idx])

    plt.tight_layout()
    plt.savefig(f"dead_relu_heatmap_{dataset_name}.png", dpi=300, bbox_inches="tight")
    plt.show()


def plot_learning_curves(results_dict, dataset_name):
    """í•™ìŠµ ê³¡ì„  ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(f"ì‹¤í—˜ B: í™œì„±í™” í•¨ìˆ˜ ë¹„êµ ê²°ê³¼ ({dataset_name})", fontsize=16)

    # í›ˆë ¨ ì†ì‹¤
    axes[0, 0].set_title("Training Loss")
    for activation_name, results in results_dict.items():
        axes[0, 0].plot(results["train_losses"], label=activation_name)
    axes[0, 0].set_xlabel("Epoch")
    axes[0, 0].set_ylabel("Loss")
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    # í…ŒìŠ¤íŠ¸ ì†ì‹¤
    axes[0, 1].set_title("Test Loss")
    for activation_name, results in results_dict.items():
        axes[0, 1].plot(results["test_losses"], label=activation_name)
    axes[0, 1].set_xlabel("Epoch")
    axes[0, 1].set_ylabel("Loss")
    axes[0, 1].legend()
    axes[0, 1].grid(True)

    # í›ˆë ¨ ì •í™•ë„
    axes[1, 0].set_title("Training Accuracy")
    for activation_name, results in results_dict.items():
        axes[1, 0].plot(results["train_accuracies"], label=activation_name)
    axes[1, 0].set_xlabel("Epoch")
    axes[1, 0].set_ylabel("Accuracy (%)")
    axes[1, 0].legend()
    axes[1, 0].grid(True)

    # í…ŒìŠ¤íŠ¸ ì •í™•ë„
    axes[1, 1].set_title("Test Accuracy")
    for activation_name, results in results_dict.items():
        axes[1, 1].plot(results["test_accuracies"], label=activation_name)
    axes[1, 1].set_xlabel("Epoch")
    axes[1, 1].set_ylabel("Accuracy (%)")
    axes[1, 1].legend()
    axes[1, 1].grid(True)

    plt.tight_layout()
    plt.savefig(
        f"experiment_b_results_{dataset_name}.png", dpi=300, bbox_inches="tight"
    )
    plt.show()


def create_comparison_table(results_dict, gradients_dict):
    """ì •ëŸ‰ì  ë¹„êµ í‘œ ìƒì„±"""
    comparison_data = []

    for activation_name, results in results_dict.items():
        final_train_acc = results["train_accuracies"][-1]
        final_test_acc = results["test_accuracies"][-1]

        # Dead ReLU ë¹„ìœ¨ (ReLU ê³„ì—´ì— ëŒ€í•´ì„œë§Œ)
        if activation_name in ["ReLU", "LeakyReLU"] and results["dead_relu_history"]:
            avg_dead_ratio = np.mean(
                [np.mean(ratios) for ratios in results["dead_relu_history"]]
            )
        else:
            avg_dead_ratio = 0.0

        # ìˆ˜ë ´ ì†ë„ (ìµœê³  ì •í™•ë„ì˜ 95%ì— ë„ë‹¬í•œ ì‹œì )
        max_acc = max(results["test_accuracies"])
        convergence_threshold = max_acc * 0.95
        convergence_epoch = next(
            (
                i
                for i, acc in enumerate(results["test_accuracies"])
                if acc >= convergence_threshold
            ),
            len(results["test_accuracies"]),
        )

        # í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°
        avg_gradient = np.mean([grad for _, grad in gradients_dict[activation_name]])

        comparison_data.append(
            {
                "í™œì„±í™” í•¨ìˆ˜": activation_name,
                "Dead ReLU ë¹„ìœ¨ (%)": f"{avg_dead_ratio:.2f}",
                "ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (%)": f"{final_test_acc:.2f}",
                "ìˆ˜ë ´ ì†ë„ (epochs)": convergence_epoch + 1,
                "í‰ê·  Gradient í¬ê¸°": f"{avg_gradient:.6f}",
            }
        )

    df = pd.DataFrame(comparison_data)
    print("\n=== ì •ëŸ‰ì  ë¹„êµ ê²°ê³¼ ===")
    print(df.to_string(index=False))

    return df


def run_experiment_b():
    """ì‹¤í—˜ B ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ì‹¤í—˜ B: í™œì„±í™” í•¨ìˆ˜ ë¹„êµ (ReLU vs LeakyReLU vs Sigmoid)")
    print("=" * 60)

    # GPU ì„¤ì •
    device, use_gpu = setup_device()
    use_amp = use_gpu  # Mixed Precisionì€ GPUì—ì„œë§Œ ì‚¬ìš©

    # ì‹¤í—˜ ì„¤ì •
    epochs = 300
    learning_rate = 0.01 if use_gpu else 0.01

    # 2D ë°ì´í„°ì…‹ ë¡œë“œ
    datasets = load_2d_datasets(use_gpu)

    # í™œì„±í™” í•¨ìˆ˜ë“¤
    activation_functions = ["relu", "leaky_relu", "sigmoid"]
    activation_names = ["ReLU", "LeakyReLU", "Sigmoid"]

    for dataset_name, data in datasets.items():
        print(f"\n--- {dataset_name.upper()} ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹¤í—˜ ---")

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_gpu_memory()

        results = {}
        gradients = {}

        for activation_fn, activation_name in zip(
            activation_functions, activation_names
        ):
            print(f"\n{activation_name} í™œì„±í™” í•¨ìˆ˜ë¡œ í•™ìŠµ ì¤‘...")

            # ëª¨ë¸ ì´ˆê¸°í™”
            model = MLPWithActivation(
                input_size=data["input_size"],
                num_classes=data["num_classes"],
                activation_fn=activation_fn,
                small_weights=True,  # Dead ReLU ìœ ë„ë¥¼ ìœ„í•œ ì‘ì€ ê°€ì¤‘ì¹˜
            )

            # ëª¨ë¸ í›ˆë ¨ ë° ë¶„ì„
            result = train_model_with_analysis(
                model, data, epochs, learning_rate, device, activation_name, use_amp
            )

            results[activation_name] = result

            # ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì„
            gradient_analysis = analyze_gradients(model, data, device)
            gradients[activation_name] = gradient_analysis

            print(f"\n{activation_name} ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì„:")
            for name, grad_val in gradient_analysis:
                print(f"  {name}: {grad_val:.6f}")

            # Dead ReLU ë¶„ì„ (ReLU ê³„ì—´ì— ëŒ€í•´ì„œë§Œ)
            if activation_name in ["ReLU", "LeakyReLU"] and result["dead_relu_history"]:
                final_dead_ratios = result["dead_relu_history"][-1]
                print(f"\n{activation_name} ìµœì¢… Dead ReLU ë¹„ìœ¨:")
                for i, ratio in enumerate(final_dead_ratios):
                    print(f"  Layer {i+1}: {ratio:.2f}%")

            # ê²°ì • ê²½ê³„ ì‹œê°í™”
            plot_2d_decision_boundary(model, data, dataset_name, activation_name)

            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            clear_gpu_memory()

        # ê²°ê³¼ ì‹œê°í™”
        plot_learning_curves(results, dataset_name)
        plot_activation_distributions(results, dataset_name)
        plot_dead_relu_heatmap(results, dataset_name)

        # ì •ëŸ‰ì  ë¹„êµ
        comparison_df = create_comparison_table(results, gradients)

        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        comparison_df.to_csv(
            f"experiment_b_comparison_{dataset_name}.csv",
            index=False,
            encoding="utf-8-sig",
        )


if __name__ == "__main__":
    # ì‹¤í—˜ B ì‹¤í–‰
    run_experiment_b()

    print("\n" + "=" * 60)
    print("ì‹¤í—˜ B ì™„ë£Œ!")
    print("ê²°ê³¼ íŒŒì¼ë“¤:")
    print("- experiment_b_results_moons.png")
    print("- experiment_b_results_circles.png")
    print("- activation_distributions_moons.png")
    print("- activation_distributions_circles.png")
    print("- dead_relu_heatmap_moons.png")
    print("- dead_relu_heatmap_circles.png")
    print("- decision_boundary_*.png")
    print("- experiment_b_comparison_moons.csv")
    print("- experiment_b_comparison_circles.csv")
    print("=" * 60)
